{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2.2 Spam Dataset\n",
    "\n",
    "In this task we want to explore email messages datasets using the SPAM Classifier introduced in the tutorial. \n",
    "First, we use the code that reads the data from the files, and creates a data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy\n",
    "from pandas import DataFrame\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "\n",
    "def progress(i, end_val, bar_length=50):\n",
    "    '''\n",
    "    Print a progress bar of the form: Percent: [#####      ]\n",
    "    i is the current progress value expected in a range [0..end_val]\n",
    "    bar_length is the width of the progress bar on the screen.\n",
    "    '''\n",
    "    percent = float(i) / end_val\n",
    "    hashes = '#' * int(round(percent * bar_length))\n",
    "    spaces = ' ' * (bar_length - len(hashes))\n",
    "    sys.stdout.write(\"\\rPercent: [{0}] {1}%\".format(hashes + spaces, int(round(percent * 100))))\n",
    "    sys.stdout.flush()\n",
    "\n",
    "NEWLINE = '\\n'\n",
    "\n",
    "HAM = 'ham'\n",
    "SPAM = 'spam'\n",
    "\n",
    "SOURCES = [\n",
    "    ('data/spam', SPAM),\n",
    "    ('data/easy_ham', HAM),\n",
    "    ('data/hard_ham', HAM),\n",
    "    ('data/beck-s', HAM),\n",
    "    ('data/farmer-d', HAM),\n",
    "    ('data/kaminski-v', HAM),\n",
    "    ('data/kitchen-l', HAM),\n",
    "    ('data/lokay-m', HAM),\n",
    "    ('data/williams-w3', HAM),\n",
    "    ('data/BG', SPAM),\n",
    "    ('data/GP', SPAM),\n",
    "    ('data/SH', SPAM)\n",
    "]\n",
    "\n",
    "SKIP_FILES = {'cmds'}\n",
    "\n",
    "\n",
    "def read_files(path):\n",
    "    '''\n",
    "    Generator of pairs (filename, filecontent)\n",
    "    for all files below path whose name is not in SKIP_FILES.\n",
    "    The content of the file is of the form:\n",
    "        header....\n",
    "        <emptyline>\n",
    "        body...\n",
    "    This skips the headers and returns body only.\n",
    "    '''\n",
    "    for root, dir_names, file_names in os.walk(path):\n",
    "        for path in dir_names:\n",
    "            read_files(os.path.join(root, path))\n",
    "        for file_name in file_names:\n",
    "            if file_name not in SKIP_FILES:\n",
    "                file_path = os.path.join(root, file_name)\n",
    "                if os.path.isfile(file_path):\n",
    "                    past_header, lines = False, []\n",
    "                    f = open(file_path, encoding=\"latin-1\")\n",
    "                    for line in f:\n",
    "                        if past_header:\n",
    "                            lines.append(line)\n",
    "                        elif line == NEWLINE:\n",
    "                            past_header = True\n",
    "                    f.close()\n",
    "                    content = NEWLINE.join(lines)\n",
    "                    yield file_path, content\n",
    "\n",
    "\n",
    "def build_data_frame(l, path, classification):\n",
    "    rows = []\n",
    "    index = []\n",
    "    for i, (file_name, text) in enumerate(read_files(path)):\n",
    "        if ((i + l) % 100 == 0):\n",
    "            progress(i + l, 58910, 50)\n",
    "        rows.append({'text': text, 'class': classification})\n",
    "        index.append(file_name)\n",
    "\n",
    "    data_frame = DataFrame(rows, index=index)\n",
    "    return data_frame, len(rows)\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    data = DataFrame({'text': [], 'class': []})\n",
    "    l = 0\n",
    "    for path, classification in SOURCES:\n",
    "        data_frame, nrows = build_data_frame(l, path, classification)\n",
    "        data = data.append(data_frame, sort=False)\n",
    "        l += nrows\n",
    "    data = data.reindex(numpy.random.permutation(data.index))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 \n",
    "\n",
    "Now we want to collect some statistics. We start by counting the number of unigrams and bigrams in the CountVectorizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent: [##################################################] 100%-----------------------------------------------\n",
      "number of unigrams and bigrams: 4015950\n"
     ]
    }
   ],
   "source": [
    "data=load_data()\n",
    "\n",
    "\n",
    "cv = CountVectorizer(ngram_range=(1, 2))\n",
    "cv.fit_transform(data[\"text\"].values)\n",
    "features = cv.get_feature_names()\n",
    "n = len(features)\n",
    "print(\"number of unigrams and bigrams: \" + str(n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2\n",
    "Next, we want to check what are the 50 most frequent unigrams and bigrams in the dataset. We'll pass the max_features argument to the CountVectorizer. That way it will  build a vocabulary that only consider the top max features ordered by term frequency across the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20', '3d', '3d http', 'align', 'and', 'arial', 'be', 'border', 'br', 'br br', 'color', 'com', 'content', 'div', 'face', 'font', 'font face', 'font size', 'for', 'height', 'href', 'html', 'http', 'http www', 'in', 'is', 'it', 'nbsp', 'nbsp nbsp', 'of', 'on', 'size', 'span', 'style', 'style 3d', 'table', 'td', 'td td', 'td tr', 'that', 'the', 'this', 'to', 'tr', 'tr td', 'width', 'with', 'www', 'you', 'your']\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(ngram_range=(1, 2),max_features=50)\n",
    "cv.fit_transform(data[\"text\"].values)\n",
    "features = cv.get_feature_names()\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.3\n",
    "\n",
    "Now, we will do the same, but this time we check the 50 most frequent unigrams and bigrams per class. FIrst, we create filtered lists, and then we pass them to the CountVectorizer fit_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 50 most frequent unigrams and bigrams per 'ham' class\n",
      "['09', '10', '20', '3d', 'an', 'and', 'are', 'as', 'at', 'be', 'br', 'by', 'com', 'ect', 'enron', 'font', 'for', 'from', 'gif', 'has', 'have', 'height', 'hou', 'http', 'http www', 'if', 'img', 'in', 'in the', 'is', 'it', 'not', 'of', 'of the', 'on', 'or', 'src', 'td', 'that', 'the', 'this', 'to', 'tr', 'we', 'width', 'will', 'with', 'www', 'you', 'your']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The 50 most frequent unigrams and bigrams per 'spam' class\n",
      "['20', '3d', '3d http', 'align', 'and', 'arial', 'border', 'br', 'br br', 'center', 'color', 'com', 'content', 'div', 'face', 'font', 'font face', 'font size', 'font td', 'for', 'height', 'href', 'href 3d', 'html', 'http', 'http www', 'in', 'is', 'nbsp', 'nbsp nbsp', 'of', 'size', 'span', 'style', 'style 3d', 'table', 'td', 'td td', 'td tr', 'text', 'the', 'this', 'to', 'tr', 'tr td', 'tr tr', 'width', 'www', 'you', 'your']\n"
     ]
    }
   ],
   "source": [
    "def filter_texts_by_class(data, class_name):\n",
    "    list = []\n",
    "    for i in range(len(data)):\n",
    "        if data[\"class\"].values[i] == class_name:\n",
    "            list.append(data[\"text\"].values[i])\n",
    "    return list\n",
    "\n",
    "print(\"The 50 most frequent unigrams and bigrams per 'ham' class\")\n",
    "filt_list = filter_texts_by_class(data,\"ham\")\n",
    "cv = CountVectorizer(ngram_range=(1, 2),max_features=50)\n",
    "cv.fit_transform(filt_list)\n",
    "features = cv.get_feature_names()\n",
    "print(features)\n",
    "print('-'*100)\n",
    "print(\"The 50 most frequent unigrams and bigrams per 'spam' class\")\n",
    "filt_list = filter_texts_by_class(data,\"spam\")\n",
    "cv = CountVectorizer(ngram_range=(1, 2),max_features=50)\n",
    "cv.fit_transform(filt_list)\n",
    "features = cv.get_feature_names()\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are some diffrences between the two sets. The 'spam' set, for example, contains more \"HTML words\" such as style,span and size. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.4\n",
    "\n",
    "Now, we want to check what are the 20 most useful features in the Naive Bayes classifier to distinguish between the two classes. First, we train the MultinomialNB classifier on the data (with tranformed text). Then we take the top 20 naive bayes coefficients. The coefficients Mirrors feature_log_prob for interpreting MultinomialNB as a linear model. The feature_log_prob is the Empirical log probability of features given a class $Pr(f_i | Class = c)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "span, style, border, face, br br, of, width, nbsp nbsp, and, http, 20, to, tr, size, the, nbsp, td, br, 3d, font\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(ngram_range=(1, 2))\n",
    "transformed_data=cv.fit_transform(data[\"text\"].values)\n",
    "clf = MultinomialNB()\n",
    "clf.fit(transformed_data, data[\"class\"].values)\n",
    "\n",
    "top20 = numpy.argsort(clf.coef_[0])[-20:]\n",
    "feature_names= cv.get_feature_names()\n",
    "topFeatures = [feature_names[j] for j in top20]\n",
    "print(\", \".join(topFeatures))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the 20-top features for the \"spam\" label. For example, the word 'nbsp' will probably appear when the class is spam. it fits 2.2.3, there we saw that nbsp si frequent in the spam texts, but not in the ham texts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
