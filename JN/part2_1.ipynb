{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2.1. Reuters Dataset \n",
    "\n",
    "## Task 2.1.1\n",
    "\n",
    "Reuters-21578 is collection for text categorization research. We want to explore how many documents are in the dataset, how many categories, how many documents per categories, provide mean and standard deviation, min and max.\n",
    "We'll use ReutersParser class (the code is from the \"out of core classification\" tutorial of Scikit-Learn). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from glob import glob\n",
    "import os.path\n",
    "import re\n",
    "import tarfile\n",
    "import sys\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import nltk\n",
    "import numpy as np\n",
    "import collections\n",
    "from sklearn.externals.six.moves import html_parser\n",
    "from sklearn.externals.six.moves.urllib.request import urlretrieve\n",
    "from sklearn.datasets import get_data_home\n",
    "\n",
    "\n",
    "\n",
    "def _not_in_sphinx():\n",
    "    # Hack to detect whether we are running by the sphinx builder\n",
    "    return '__file__' in globals()\n",
    "\n",
    "\n",
    "class ReutersParser(html_parser.HTMLParser):\n",
    "    \"\"\"Utility class to parse a SGML file and yield documents one at a time.\"\"\"\n",
    "\n",
    "    def __init__(self, encoding='latin-1'):\n",
    "        html_parser.HTMLParser.__init__(self)\n",
    "        self._reset()\n",
    "        self.encoding = encoding\n",
    "\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        method = 'start_' + tag\n",
    "        getattr(self, method, lambda x: None)(attrs)\n",
    "\n",
    "    def handle_endtag(self, tag):\n",
    "        method = 'end_' + tag\n",
    "        getattr(self, method, lambda: None)()\n",
    "\n",
    "    def _reset(self):\n",
    "        self.in_title = 0\n",
    "        self.in_body = 0\n",
    "        self.in_topics = 0\n",
    "        self.in_topic_d = 0\n",
    "        self.title = \"\"\n",
    "        self.body = \"\"\n",
    "        self.topics = []\n",
    "        self.topic_d = \"\"\n",
    "\n",
    "    def parse(self, fd):\n",
    "        self.docs = []\n",
    "        for chunk in fd:\n",
    "            self.feed(chunk.decode(self.encoding))\n",
    "            for doc in self.docs:\n",
    "                yield doc\n",
    "            self.docs = []\n",
    "        self.close()\n",
    "\n",
    "    def handle_data(self, data):\n",
    "        if self.in_body:\n",
    "            self.body += data\n",
    "        elif self.in_title:\n",
    "            self.title += data\n",
    "        elif self.in_topic_d:\n",
    "            self.topic_d += data\n",
    "\n",
    "    def start_reuters(self, attributes):\n",
    "        pass\n",
    "\n",
    "    def end_reuters(self):\n",
    "        self.body = re.sub(r'\\s+', r' ', self.body)\n",
    "        self.docs.append({'title': self.title,\n",
    "                          'body': self.body,\n",
    "                          'topics': self.topics})\n",
    "        self._reset()\n",
    "\n",
    "    def start_title(self, attributes):\n",
    "        self.in_title = 1\n",
    "\n",
    "    def end_title(self):\n",
    "        self.in_title = 0\n",
    "\n",
    "    def start_body(self, attributes):\n",
    "        self.in_body = 1\n",
    "\n",
    "    def end_body(self):\n",
    "        self.in_body = 0\n",
    "\n",
    "    def start_topics(self, attributes):\n",
    "        self.in_topics = 1\n",
    "\n",
    "    def end_topics(self):\n",
    "        self.in_topics = 0\n",
    "\n",
    "    def start_d(self, attributes):\n",
    "        self.in_topic_d = 1\n",
    "\n",
    "    def end_d(self):\n",
    "        self.in_topic_d = 0\n",
    "        self.topics.append(self.topic_d)\n",
    "        self.topic_d = \"\"\n",
    "\n",
    "\n",
    "def stream_reuters_documents(data_path=None):\n",
    "    DOWNLOAD_URL = ('http://archive.ics.uci.edu/ml/machine-learning-databases/'\n",
    "                    'reuters21578-mld/reuters21578.tar.gz')\n",
    "    ARCHIVE_FILENAME = 'reuters21578.tar.gz'\n",
    "\n",
    "    if data_path is None:\n",
    "        data_path = os.path.join(get_data_home(), \"reuters\")\n",
    "    if not os.path.exists(data_path):\n",
    "        \"\"\"Download the dataset.\"\"\"\n",
    "        print(\"downloading dataset (once and for all) into %s\" %\n",
    "              data_path)\n",
    "        os.mkdir(data_path)\n",
    "\n",
    "        def progress(blocknum, bs, size):\n",
    "            total_sz_mb = '%.2f MB' % (size / 1e6)\n",
    "            current_sz_mb = '%.2f MB' % ((blocknum * bs) / 1e6)\n",
    "            if _not_in_sphinx():\n",
    "                sys.stdout.write(\n",
    "                    '\\rdownloaded %s / %s' % (current_sz_mb, total_sz_mb))\n",
    "\n",
    "        archive_path = os.path.join(data_path, ARCHIVE_FILENAME)\n",
    "        urlretrieve(DOWNLOAD_URL, filename=archive_path,\n",
    "                    reporthook=progress)\n",
    "        if _not_in_sphinx():\n",
    "            sys.stdout.write('\\r')\n",
    "        print(\"untarring Reuters dataset...\")\n",
    "        tarfile.open(archive_path, 'r:gz').extractall(data_path)\n",
    "        print(\"done.\")\n",
    "\n",
    "    parser = ReutersParser()\n",
    "    for filename in glob(os.path.join(data_path, \"*.sgm\")):\n",
    "        for doc in parser.parse(open(filename, 'rb')):\n",
    "            yield doc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use the parser in order to load the documents labels into a list. the i-th element in the list, is a list contains the labels of the i-th document: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_stream = stream_reuters_documents()\n",
    "\n",
    "\n",
    "def get_docs_topics(doc_iter):\n",
    "    data = [doc['topics']\n",
    "            for doc in doc_iter]\n",
    "    if not len(data):\n",
    "        return np.asarray([], dtype=int), np.asarray([], dtype=int)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a dictionary with categories as keys, and the number of documents labeled by this category as values.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = get_docs_topics(data_stream)\n",
    "topics_dict ={}\n",
    "for doc_topics in topics:\n",
    "    for top in doc_topics:\n",
    "        if top in topics_dict:\n",
    "            topics_dict[top] += 1\n",
    "        else:\n",
    "            topics_dict[top] = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can collect some statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents is: 21578\n",
      "Number of categories is: 445\n",
      "Documents per categories: 48.48988764044944\n",
      "Mean of docs per categories: 89.87191011235954\n",
      "STD of docs per categories: 643.9321684195976\n",
      "Max category: [('usa', 12542)]\n",
      "Min category: ('stich', 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of documents is: \" + str(len(topics)))\n",
    "print(\"Number of categories is: \" + str(len(topics_dict)))\n",
    "print(\"Documents per categories: \" + str((len(topics))/(len(topics_dict))))\n",
    "np_topics = np.array([])\n",
    "for i in topics_dict.values():\n",
    "    np_topics = np.append(np_topics,i)\n",
    "\n",
    "print(\"Mean of docs per categories: \" + str(np.mean(np_topics)))\n",
    "print(\"STD of docs per categories: \" + str(np.std(np_topics)))\n",
    "\n",
    "topics_dict = collections.Counter(topics_dict)\n",
    "print(\"Max category: \" + str(topics_dict.most_common(1)))\n",
    "\n",
    "print(\"Min category: \" + str(topics_dict.most_common()[len(topics_dict)-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.1.2\n",
    "We want to explore how many characters and words are present in the documents of the dataset. For this purpose, we collect all the contents of the documents, concatenate them and clean tags. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_stream = stream_reuters_documents()\n",
    "\n",
    "def get_docs_body(doc_iter):\n",
    "\n",
    "    data = [(u'{title}\\n\\n{body}'.format(**doc))\n",
    "            for doc in doc_iter\n",
    "            if doc['topics']]\n",
    "    if not len(data):\n",
    "        return np.asarray([], dtype=int), np.asarray([], dtype=int)\n",
    "    return data\n",
    "\n",
    "\n",
    "texts = get_docs_body(data_stream)\n",
    "all_text = \"\"\n",
    "for txt in texts:\n",
    "    all_text = all_text + txt\n",
    "\n",
    "clean_text = bs(all_text).get_text()\n",
    "words = nltk.word_tokenize(clean_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we count the total number of words in our concatenated string, and the total number of distinct words. \n",
    "We do the same for characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the dataset: 2881884\n",
      "Number of different words in the dataset: 83405\n",
      "Number of chars in the dataset: 13213960\n",
      "Number of different chars in the dataset: 89\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of words in the dataset: \" + str(len(words)))\n",
    "print(\"Number of different words in the dataset: \" + str(len(set(words))))\n",
    "\n",
    "ch_list = []\n",
    "for w in words:\n",
    "    for ch in w:\n",
    "        ch_list.append(ch)\n",
    "\n",
    "print(\"Number of chars in the dataset: \" + str(len(ch_list)))\n",
    "print(\"Number of different chars in the dataset: \" + str(len(set(ch_list))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.1.3\n",
    "\n",
    "The classifiers that support the \"partial-fit\" method disscused in the tutorial code are:\n",
    "1. SGD Classifier\n",
    "2. Perceptron\n",
    "3. NB Multinomial\n",
    "4.  Passive-Aggressive Classifier\n",
    "\n",
    "partial_fit method allows minibatch learning. It can be used whenever the dataset is too big. Then, we can perform our fitting in smaller batches and each batch is used to extend the existing classifier. In the tutorial code the learning algorithm takes 1000 documents at each step, and then it execute patiral_fit method. Each new batch extends the classification, and it is reflected in the charts at the end of the tutorial - the more examples that we see (and therefore the number of batches), the better the accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.1.4\n",
    "\n",
    "The hashing vectorizer converts a collection of text documents to a matrix of token occurrences. It uses hashing in order to find the token string name to feature integer index mapping. Hash functions are an efficient way of mapping terms to features, and the ashing vectorizercan be used in a streaming (partial fit), since it guarantees that the features space remains the same over time (due to increased collision)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
